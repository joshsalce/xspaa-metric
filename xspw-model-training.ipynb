{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10698103,"sourceType":"datasetVersion","datasetId":6623893}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nimport optuna\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import log_loss, brier_score_loss\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import load_model\n\npath = \"/kaggle/input/xspaa-data/\"\noutput_path = \"/kaggle/working/\"\n\nwarnings.filterwarnings(\"ignore\")\n\ntf.random.set_seed(909)\nnp.random.seed(909)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.531174Z","iopub.execute_input":"2025-02-08T22:50:52.531585Z","iopub.status.idle":"2025-02-08T22:50:52.566500Z","shell.execute_reply.started":"2025-02-08T22:50:52.531551Z","shell.execute_reply":"2025-02-08T22:50:52.565448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read in Data\ntrain_data = pd.read_csv(path + \"training_set.csv\")\ntest_data = pd.read_csv(path + \"test_set.csv\")\npoints_data = pd.read_csv(path + \"points_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.568543Z","iopub.execute_input":"2025-02-08T22:50:52.568835Z","iopub.status.idle":"2025-02-08T22:50:52.587558Z","shell.execute_reply.started":"2025-02-08T22:50:52.568810Z","shell.execute_reply":"2025-02-08T22:50:52.586001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set X and y columns\nxgb_model_data_cols = ['stroke', 'type_of_shot', 'server_x', 'server_y', 'receiver_x', 'receiver_y']\n\ntrain_X = train_data[xgb_model_data_cols].astype({\"stroke\":'category', \"type_of_shot\":'category'}) \ntrain_y = train_data[\"ServerWinsPoint\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.589532Z","iopub.execute_input":"2025-02-08T22:50:52.589901Z","iopub.status.idle":"2025-02-08T22:50:52.601499Z","shell.execute_reply.started":"2025-02-08T22:50:52.589862Z","shell.execute_reply":"2025-02-08T22:50:52.600512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline","metadata":{}},{"cell_type":"code","source":"# Create a copy of test_data to maintain a baseline\nbaseline_test_data = test_data.copy()\n\n# Define the baseline prediction probability for server winning point\nbaseline_pred_proba = 0.642\n\ntest_rally_outcomes = baseline_test_data[[\"rallyid\", \"ServerWinsPoint\"]].drop_duplicates()\n\n# Add pred, probability columns\ntest_rally_outcomes[\"pred\"] = round(baseline_pred_proba)\ntest_rally_outcomes[\"prob\"] = baseline_pred_proba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.602708Z","iopub.execute_input":"2025-02-08T22:50:52.603059Z","iopub.status.idle":"2025-02-08T22:50:52.618808Z","shell.execute_reply.started":"2025-02-08T22:50:52.603025Z","shell.execute_reply":"2025-02-08T22:50:52.617776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate Log Loss and Brier Score for points\nbaseline_outcome_lloss = log_loss(test_rally_outcomes[\"ServerWinsPoint\"], test_rally_outcomes[\"pred\"])\nbaseline_outcome_brier = brier_score_loss(test_rally_outcomes[\"ServerWinsPoint\"], test_rally_outcomes[\"prob\"])\n\nprint(f\"Baseline Outcome Model Log Loss: {baseline_outcome_lloss}\")\nprint(f\"Baseline Outcome Model Brier Score: {baseline_outcome_brier}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.620208Z","iopub.execute_input":"2025-02-08T22:50:52.620584Z","iopub.status.idle":"2025-02-08T22:50:52.640949Z","shell.execute_reply.started":"2025-02-08T22:50:52.620552Z","shell.execute_reply":"2025-02-08T22:50:52.639879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate xSPW values, sums from taking outcome - probability\ntest_rally_outcomes[\"xSPW\"] = test_rally_outcomes[\"ServerWinsPoint\"] - test_rally_outcomes[\"prob\"]\nbaseline_xSPW = pd.merge(points_data[[\"rallyid\", \"server\"]], test_rally_outcomes).groupby(\"server\").xSPW.sum().reset_index()\nbaseline_xSPW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.642103Z","iopub.execute_input":"2025-02-08T22:50:52.642423Z","iopub.status.idle":"2025-02-08T22:50:52.667127Z","shell.execute_reply.started":"2025-02-08T22:50:52.642398Z","shell.execute_reply":"2025-02-08T22:50:52.666064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"\n    Purpose: Create objective function to optimize model hyperparameters using Optuna.\n\n    Input(s):\n        trial (optuna.trial.Trial): A trial object that suggests hyperparameters.\n\n    Output(s):\n        cv_logloss (float): Negative mean log loss score from cross-validation.\n    \"\"\"\n\n    # Define the parameter space for XGBClassifier\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"reg:squarederror\",\n        \"booster\": \"gbtree\",\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n    }\n    \n    # Train XGBClassifier with the defined parameters and evaluate using log loss\n    model = xgb.XGBClassifier(**param, eval_metric='logloss', use_label_encoder=True, enable_categorical=True)\n\n    # Set up KFold cross-validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=909)\n    logloss_scores = cross_val_score(model, train_X, train_y, scoring='neg_log_loss', cv=kf, n_jobs=-1)\n\n    cv_logloss = -np.mean(logloss_scores)\n    return cv_logloss\n\n# Run the optimization study with direction to minimize the objective function\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(f\"Best Hyperparameters: {study.best_params}\")\nbest_params = study.best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:50:52.668802Z","iopub.execute_input":"2025-02-08T22:50:52.669129Z","iopub.status.idle":"2025-02-08T22:51:15.502829Z","shell.execute_reply.started":"2025-02-08T22:50:52.669103Z","shell.execute_reply":"2025-02-08T22:51:15.501652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train final model with best params\nxgb_model = xgb.XGBClassifier(**best_params, use_label_encoder = True, enable_categorical = True)\nxgb_model.fit(train_X, train_y)\n\nplot_importance(xgb_model, importance_type='weight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:51:15.504552Z","iopub.execute_input":"2025-02-08T22:51:15.504958Z","iopub.status.idle":"2025-02-08T22:51:15.812022Z","shell.execute_reply.started":"2025-02-08T22:51:15.504920Z","shell.execute_reply":"2025-02-08T22:51:15.810998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Repeat process for XGBoost, predicting only with final rally\ntest_data_final_stroke = test_data.groupby(\"rallyid\").strokeid.max().reset_index()\ntest_X_xgb = pd.merge(test_data, test_data_final_stroke)[xgb_model_data_cols].astype({\"stroke\":'category', \"type_of_shot\":'category'}) \ntest_y_xgb = test_data[[\"rallyid\", \"ServerWinsPoint\"]].drop_duplicates().reset_index(drop = True)\n\ntest_y_outcomes = test_data.ServerWinsPoint\npred_y = xgb_model.predict(test_X_xgb)\npred_proba_y = xgb_model.predict_proba(test_X_xgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:51:15.813887Z","iopub.execute_input":"2025-02-08T22:51:15.814179Z","iopub.status.idle":"2025-02-08T22:51:15.841500Z","shell.execute_reply.started":"2025-02-08T22:51:15.814153Z","shell.execute_reply":"2025-02-08T22:51:15.840619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caluclate Log Loss and Brier Score\nxgboost_lloss = log_loss(test_y_xgb.ServerWinsPoint, pred_y)\nxgboost_brier = brier_score_loss(test_y_xgb.ServerWinsPoint, pred_proba_y[:, 1])\n\nprint(f\"XGBoost Event Model Log Loss: {xgboost_lloss}\")\nprint(f\"XGBoost Event Model Brier Score: {xgboost_brier}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:51:15.842326Z","iopub.execute_input":"2025-02-08T22:51:15.842634Z","iopub.status.idle":"2025-02-08T22:51:15.860396Z","shell.execute_reply.started":"2025-02-08T22:51:15.842605Z","shell.execute_reply":"2025-02-08T22:51:15.859393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate xSPW values, sums\ntest_y_xgb[\"prob\"] = pred_proba_y[:, 1]\ntest_y_xgb[\"xSPW\"] = test_y_xgb[\"ServerWinsPoint\"] - test_y_xgb[\"prob\"]\n\nxgb_xSPW = pd.merge(points_data[[\"rallyid\", \"server\"]], test_y_xgb).groupby(\"server\").xSPW.sum().reset_index()\nxgb_xSPW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:51:15.861377Z","iopub.execute_input":"2025-02-08T22:51:15.861757Z","iopub.status.idle":"2025-02-08T22:51:15.883022Z","shell.execute_reply.started":"2025-02-08T22:51:15.861729Z","shell.execute_reply":"2025-02-08T22:51:15.882006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Nets","metadata":{}},{"cell_type":"code","source":"# Set X and y columns\nnnet_model_data_cols = [\"rallyid\", \"strokeid\", 'stroke', 'type_of_shot', 'time_diff','server_x', 'server_y', 'receiver_x', 'receiver_y',\n                   'server_distance_from_baseline', 'receiver_distance_from_baseline']\n\ntrain_X = train_data[nnet_model_data_cols].astype({\"stroke\":'category', \"type_of_shot\":'category'}) \ntest_X = test_data[nnet_model_data_cols].astype({\"stroke\":'category', \"type_of_shot\":'category'}) \n\ntrain_y = train_data[[\"rallyid\", \"ServerWinsPoint\"]].drop_duplicates().reset_index(drop = True)\ntest_y = test_data[[\"rallyid\", \"ServerWinsPoint\"]].drop_duplicates().reset_index(drop = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:23:11.393360Z","iopub.execute_input":"2025-02-08T22:23:11.393638Z","iopub.status.idle":"2025-02-08T22:23:11.420288Z","shell.execute_reply.started":"2025-02-08T22:23:11.393615Z","shell.execute_reply":"2025-02-08T22:23:11.419294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_ohc_transformation(data, cols_to_encode, one_hot_encoder = None):\n    \"\"\"\n    Purpose: With raw dataset, transform input columns with one-hot encoder\n\n    Input(s): \n        data (pd.DataFrame): Raw dataset\n        cols_to_encode (list): contains names of column in input\n        one_hot_encoder (NoneType or Sklearn object): default None, creates or uses preexisting one-hot encoder object\n\n    Output(s):\n        preprocessed_data (NumPy array): Dataset with one-hot encoded columns\n        one_hot_encoder (Sklearn object): Newly created or existing OHC object\n    \"\"\"\n\n    # Check if OHC object exists\n    if one_hot_encoder is None:\n        # Initialize OHC Object\n        one_hot_encoder = OneHotEncoder(sparse=False, drop = \"first\", handle_unknown = \"ignore\")\n\n        # Fit the encoder to the specified columns\n        encoded_columns = one_hot_encoder.fit_transform(data[cols_to_encode])\n\n    else:\n        # Fit the encoder to the specified columns\n        encoded_columns = one_hot_encoder.transform(data[cols_to_encode])\n        \n    # Convert encoded columns to DataFrame\n    encoded_cols = pd.DataFrame(encoded_columns, columns=one_hot_encoder.get_feature_names_out(cols_to_encode))\n        \n    # Concatenate encoded columns with the original DataFrame\n    preprocessed_data = pd.concat([data.drop(columns=cols_to_encode), encoded_cols], axis=1)\n    \n    return(preprocessed_data, one_hot_encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:23:11.421283Z","iopub.execute_input":"2025-02-08T22:23:11.421575Z","iopub.status.idle":"2025-02-08T22:23:11.427903Z","shell.execute_reply.started":"2025-02-08T22:23:11.421551Z","shell.execute_reply":"2025-02-08T22:23:11.426962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_nnet_data(x_train, y_train, ohc_cols, fit_ohc=None):\n    \"\"\"\n    Purpose: Prepare the data for training an LSTM model via OHC and segmenting data into lists of arrays\n\n    Input(s):\n        x_train (pd.DataFrame): Contains training data\n        y_train (pd.DataFrame): Contains training data labels\n        ohc_cols (list): List of columns to be one-hot encoded\n        fit_ohc (OneHotEncoder): Pre-fitted OneHotEncoder. Default None, creates a new encoder if None\n\n    Output(s):\n        tuple: A tuple containing preprocessed data with padding (X, y), plus fitted OneHotEncoder.\n    \"\"\"\n    \n    # Perform one-hot encoding\n    if fit_ohc is None:\n        X_tr, encoder = perform_ohc_transformation(x_train, ohc_cols)\n    else:\n        X_tr, encoder = perform_ohc_transformation(x_train, ohc_cols, one_hot_encoder=fit_ohc)\n\n\n    # Prepare the data for LSTM\n    X = []\n    y = []\n    rallies = X_tr[\"rallyid\"].unique()\n\n    # Iterating through unique rallies\n    for rally in rallies:\n        # Select all events\n        event_data = X_tr[X_tr[\"rallyid\"] == rally]\n        outcome = y_train[y_train[\"rallyid\"] == rally]\n\n        # Convert data into own array, add to list of data\n        X.append(event_data.drop([\"rallyid\", \"strokeid\"], axis=1).values)\n        y.append(outcome[\"ServerWinsPoint\"].values[0])\n\n    y = np.array(y)\n    return (X, y, encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:23:11.428843Z","iopub.execute_input":"2025-02-08T22:23:11.429260Z","iopub.status.idle":"2025-02-08T22:23:11.447688Z","shell.execute_reply.started":"2025-02-08T22:23:11.429223Z","shell.execute_reply":"2025-02-08T22:23:11.446774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Specify columns to be encoded\ncolumns_to_encode = ['type_of_shot', 'stroke']\nX_nnet, y_nnet, ohc = prepare_nnet_data(train_X, train_y, columns_to_encode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:23:11.448606Z","iopub.execute_input":"2025-02-08T22:23:11.448952Z","iopub.status.idle":"2025-02-08T22:23:11.578803Z","shell.execute_reply.started":"2025-02-08T22:23:11.448919Z","shell.execute_reply":"2025-02-08T22:23:11.578008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define number of timestamps, features to use in LSTM model\nmax_strokes_per_rally = train_X.groupby(\"rallyid\").strokeid.count().reset_index().strokeid.max()\nnum_features = len(X_nnet[0][0])\n\n# Pad preprocessed data to go up to max number of timestamps \nX_nnet_padded = pad_sequences(X_nnet, padding = 'post', dtype = 'float32', value = -100, maxlen = max_strokes_per_rally)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:23:11.579742Z","iopub.execute_input":"2025-02-08T22:23:11.580017Z","iopub.status.idle":"2025-02-08T22:23:11.587545Z","shell.execute_reply.started":"2025-02-08T22:23:11.579994Z","shell.execute_reply":"2025-02-08T22:23:11.586622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform 5-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=909)\nfold = 1\n\nlog_loss_scores = []\n\n# Iterating through each fold\nfor train_index, test_index in kf.split(X_nnet_padded):\n    print(f'Fold {fold}')\n    X_train, X_test = X_nnet_padded[train_index], X_nnet_padded[test_index]\n    y_train, y_test = y_nnet[train_index], y_nnet[test_index]\n\n    # Defining the LSTM Sequential model (simple, heavily regularized)\n    model = Sequential()\n    model.add(LSTM(5, activation='relu', recurrent_dropout=0.1, \n              kernel_regularizer=l2(0.01), input_shape = (max_strokes_per_rally, num_features)))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Early stopping callback\n    early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta = 0.5,restore_best_weights=True)\n    \n    # Train the model with callback\n    model.fit(X_train, y_train, epochs=25, batch_size=16, callbacks=[early_stopping])\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(X_test, y_test)\n    print(f'Test Accuracy for fold {fold}: {accuracy * 100:.2f}%')\n    \n    # Predict the outcome\n    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n    print(f'Predicted outcomes for fold {fold}: {y_pred.flatten()}')\n\n    log_loss_scores.append(loss)\n    fold += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:25:51.316180Z","iopub.execute_input":"2025-02-08T22:25:51.316623Z","iopub.status.idle":"2025-02-08T22:26:07.931902Z","shell.execute_reply.started":"2025-02-08T22:25:51.316587Z","shell.execute_reply":"2025-02-08T22:26:07.931069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Log-Loss as mean across all folds\nprint(f\"LSTM Model, Mean CV Log-Loss: {np.mean(log_loss_scores)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:26:07.933830Z","iopub.execute_input":"2025-02-08T22:26:07.934114Z","iopub.status.idle":"2025-02-08T22:26:07.938955Z","shell.execute_reply.started":"2025-02-08T22:26:07.934091Z","shell.execute_reply":"2025-02-08T22:26:07.938049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Final Model, Get Predictions","metadata":{}},{"cell_type":"code","source":"# Retrain model using all sequences\nmodel = Sequential()\nmodel.add(LSTM(5, activation='relu', recurrent_dropout=0.1, \n              kernel_regularizer=l2(0.01), input_shape = (max_strokes_per_rally, num_features)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(monitor='loss', patience=5, min_delta = 0.5, restore_best_weights=True)\n    \nmodel.fit(X_nnet_padded, y_nnet, epochs=25, batch_size=16, callbacks=[early_stopping])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:54:49.548039Z","iopub.execute_input":"2025-02-08T22:54:49.548426Z","iopub.status.idle":"2025-02-08T22:54:53.324659Z","shell.execute_reply.started":"2025-02-08T22:54:49.548391Z","shell.execute_reply":"2025-02-08T22:54:53.323684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess test set\ntest_X_nnet, test_y_nnet, ohc = prepare_nnet_data(test_X, test_y, columns_to_encode, ohc)\ntest_X_nnet_padded = pad_sequences(test_X_nnet, padding = 'post', dtype = 'float32', value = -100, maxlen = max_strokes_per_rally)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:26:18.956520Z","iopub.execute_input":"2025-02-08T22:26:18.956846Z","iopub.status.idle":"2025-02-08T22:26:18.997362Z","shell.execute_reply.started":"2025-02-08T22:26:18.956822Z","shell.execute_reply":"2025-02-08T22:26:18.996597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate test set preds, Log Loss, Brier Score\ntest_y[\"prob\"] = model.predict(test_X_nnet_padded)\ntest_y[\"xSPW\"] = test_y.ServerWinsPoint - test_y.prob\n\nprint(f\"LSTM Model Log-Loss: {log_loss(test_y['ServerWinsPoint'], test_y['prob'])}\")\nprint(f\"LSTM Model Brier Score: {brier_score_loss(test_y['ServerWinsPoint'], test_y['prob'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:54:59.936479Z","iopub.execute_input":"2025-02-08T22:54:59.936814Z","iopub.status.idle":"2025-02-08T22:55:00.173860Z","shell.execute_reply.started":"2025-02-08T22:54:59.936789Z","shell.execute_reply":"2025-02-08T22:55:00.172689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Recalculate xSPW values, sums\nplayer_xSPW = pd.merge(points_data[[\"rallyid\", \"server\"]], test_y)\nplayer_summary_final_games = player_xSPW.groupby(\"server\").xSPW.sum().reset_index()\nplayer_summary_final_games","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:55:01.975226Z","iopub.execute_input":"2025-02-08T22:55:01.975659Z","iopub.status.idle":"2025-02-08T22:55:01.990193Z","shell.execute_reply.started":"2025-02-08T22:55:01.975625Z","shell.execute_reply":"2025-02-08T22:55:01.989175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\nmodel.save(output_path + 'xspw_prototype.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:26:31.831070Z","iopub.execute_input":"2025-02-08T22:26:31.831414Z","iopub.status.idle":"2025-02-08T22:26:31.860229Z","shell.execute_reply.started":"2025-02-08T22:26:31.831386Z","shell.execute_reply":"2025-02-08T22:26:31.859345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_cumulative_sequences(data, length, n, m):\n    \"\"\"\n    Purpose: Create cumulative sequences of event data with padding\n\n    Input(s):\n        data (list): A list of sequences containing event data\n        length (int): The maximum length of the sequences after padding\n        n (int): The number of sequences\n        m (int): The number of features in each sequence\n\n    Output(s):\n        events_component (np.ndarray): An array of padded cumulative event sequences with the shape (n, length, m).\n    \"\"\"\n    \n    # Initialize list to hold cumulative event sequences\n    event_components = []\n\n    # Iterate through each sequence in the data\n    for ind in range(len(data)):\n        for event_ind in range(len(data[ind])):\n            # Create a cumulative sequence up to the current event\n            sequence = [data[ind][:event_ind + 1]]\n            \n            # Pad the sequence to the specified length\n            padded_sequence = pad_sequences(sequence, padding='post', dtype='float32', value=-100, maxlen=length)\n            event_components.append(padded_sequence)\n    \n    # Reshape the event components to the desired shape\n    event_components = np.array(event_components).reshape(n, length, m)\n\n    return event_components","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:26:33.390050Z","iopub.execute_input":"2025-02-08T22:26:33.390413Z","iopub.status.idle":"2025-02-08T22:26:33.396423Z","shell.execute_reply.started":"2025-02-08T22:26:33.390381Z","shell.execute_reply":"2025-02-08T22:26:33.395247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert test set into list of cumulative padded sequences for each event\nnum_samples = test_X.shape[0]\nn_features = test_X_nnet[0].shape[1]\n\nX_test_sequential = create_cumulative_sequences(test_X_nnet, max_strokes_per_rally, num_samples, n_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:26:35.626536Z","iopub.execute_input":"2025-02-08T22:26:35.626964Z","iopub.status.idle":"2025-02-08T22:26:35.636223Z","shell.execute_reply.started":"2025-02-08T22:26:35.626928Z","shell.execute_reply":"2025-02-08T22:26:35.635089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For each sequence, predict probabilty of server winning at each event\npredict_df = test_X.copy()\npredict_df[\"prob\"] = model.predict(X_test_sequential)\n\n#predict_df.to_csv(\"event_predictions.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T22:27:11.929569Z","iopub.execute_input":"2025-02-08T22:27:11.929925Z","iopub.status.idle":"2025-02-08T22:27:12.024328Z","shell.execute_reply.started":"2025-02-08T22:27:11.929900Z","shell.execute_reply":"2025-02-08T22:27:12.023534Z"}},"outputs":[],"execution_count":null}]}