{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe83114",
   "metadata": {
    "papermill": {
     "duration": 0.005602,
     "end_time": "2025-02-05T00:13:13.788232",
     "exception": false,
     "start_time": "2025-02-05T00:13:13.782630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e478867",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:13.799564Z",
     "iopub.status.busy": "2025-02-05T00:13:13.799096Z",
     "iopub.status.idle": "2025-02-05T00:13:17.603877Z",
     "shell.execute_reply": "2025-02-05T00:13:17.602636Z"
    },
    "papermill": {
     "duration": 3.812638,
     "end_time": "2025-02-05T00:13:17.605981",
     "exception": false,
     "start_time": "2025-02-05T00:13:13.793343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "path = \"/kaggle/input/tennis-atp-tour-australian-open-final-2019/\"\n",
    "output_path = \"/kaggle/working/\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd68200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.618123Z",
     "iopub.status.busy": "2025-02-05T00:13:17.617578Z",
     "iopub.status.idle": "2025-02-05T00:13:17.679603Z",
     "shell.execute_reply": "2025-02-05T00:13:17.678383Z"
    },
    "papermill": {
     "duration": 0.070363,
     "end_time": "2025-02-05T00:13:17.681786",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.611423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/tennis-atp-tour-australian-open-final-2019/events.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import all data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m events \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m points \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m serves \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserves.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/tennis-atp-tour-australian-open-final-2019/events.csv'"
     ]
    }
   ],
   "source": [
    "# Import all data\n",
    "events = pd.read_csv(path + \"events.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "points = pd.read_csv(path + \"points.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "serves = pd.read_csv(path + \"serves.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "rallies = pd.read_csv(path + \"rallies.csv\").drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5ea60",
   "metadata": {
    "papermill": {
     "duration": 0.004705,
     "end_time": "2025-02-05T00:13:17.691651",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.686946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Cleaning, Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6ba28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.702685Z",
     "iopub.status.busy": "2025-02-05T00:13:17.702302Z",
     "iopub.status.idle": "2025-02-05T00:13:17.708469Z",
     "shell.execute_reply": "2025-02-05T00:13:17.707551Z"
    },
    "papermill": {
     "duration": 0.013589,
     "end_time": "2025-02-05T00:13:17.710092",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.696503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_true_rally_times(events_data, points_data = points):\n",
    "    \"\"\"\n",
    "    Purpose: Clean points dataset to get true rally times from events dataset\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event-level data for each rally\n",
    "        points_data (pd.DataFrame): Contains data at the level of each rally\n",
    "\n",
    "    Output(s):\n",
    "        points_data_clean (pd.DataFrame): points dataset with totaltime column changed\n",
    "    \"\"\"\n",
    "\n",
    "    # Group by rally and returning min, max time\n",
    "    points_minmax_times = events_data.groupby(\"rallyid\").time.agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "    # Create elapsed time variable from min and max time difference\n",
    "    points_minmax_times[\"elapsed_time\"] = points_minmax_times[\"max\"] - points_minmax_times[\"min\"]\n",
    "\n",
    "    # Join times to points dataframe \n",
    "    points_data_clean = pd.merge(points_data, points_minmax_times[[\"rallyid\", \"elapsed_time\"]]).drop(\"totaltime\", axis = 1).rename(columns = {\"elapsed_time\": \"totaltime\"})\n",
    "    points_data_clean = points_data_clean[points_data_clean.rallyid != 101]\n",
    "    \n",
    "    return points_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71226a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.721682Z",
     "iopub.status.busy": "2025-02-05T00:13:17.721253Z",
     "iopub.status.idle": "2025-02-05T00:13:17.730359Z",
     "shell.execute_reply": "2025-02-05T00:13:17.729369Z"
    },
    "papermill": {
     "duration": 0.016624,
     "end_time": "2025-02-05T00:13:17.731936",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.715312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_rally_hitter_receiver_names(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Clean events dataset to fill in missing hitter and receiver column values\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event-level data for each rally\n",
    "\n",
    "    Output(s):\n",
    "        events_data_clean (pd.DataFrame): events dataset with hitter/receiver column values changed    \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with cleaned rally names for specific rally and stroke IDs\n",
    "    cleaned_rally_names = pd.DataFrame({\n",
    "        \"rallyid\": [81, 110, 115, 155, 156, 12, 21, 29, 135, 149, 152, 154, 159, 160, 160, 160, 161, 168, 175, 176, 194, 196, 196, 196, 196],\n",
    "        \"strokeid\": [3, 7, 8, 8, 4, 1, 1, 1, 7, 2, 11, 14, 1, 1, 2, 3, 7, 5, 2, 8, 13, 1, 2, 3, 4], \n",
    "        \"hitter\": [\"Nadal\", \"Djokovic\", \"Nadal\", \"Djokovic\", \"Nadal\", \"Nadal\", \"Nadal\", \"Nadal\", \"Nadal\", \"Djokovic\", \"Nadal\", \"Djokovic\", \"Djokovic\", \"Djokovic\", \n",
    "                   \"Nadal\", \"Djokovic\", \"Djokovic\", \"Nadal\", \"Nadal\", \"Nadal\", \"Djokovic\", \"Djokovic\", \"Nadal\", \"Djokovic\", \"Nadal\"], \n",
    "        \"receiver\": [\"Djokovic\", \"Nadal\", \"Djokovic\", \"Nadal\", \"Djokovic\", \"Djokovic\", \"Djokovic\", \"Djokovic\", \"Djokovic\", \"Nadal\", \"Djokovic\", \"Nadal\", \"Nadal\", \"Nadal\", \n",
    "                     \"Djokovic\", \"Nadal\", \"Nadal\", \"Djokovic\", \"Djokovic\", \"Djokovic\", \"Nadal\", \"Nadal\", \"Djokovic\", \"Nadal\", \"Djokovic\"]\n",
    "    })\n",
    "\n",
    "    # Merge the events data with the cleaned rally names on rallyid and strokeid\n",
    "    events_merge = pd.merge(events_data, cleaned_rally_names, how=\"left\", on=[\"rallyid\", \"strokeid\"], suffixes=(\"_old\", \"_new\"))\n",
    "\n",
    "    # Update the hitter column with old hitter values\n",
    "    events_merge[\"hitter\"] = events_merge.hitter_old\n",
    "    \n",
    "    # Update the receiver column based on certain conditions\n",
    "    events_merge[\"receiver\"] = np.where((events_merge.hitter_old == events_merge.receiver_old) | (events_merge.receiver_old.isna()), \n",
    "                                        events_merge.receiver_new, events_merge.receiver_old)\n",
    "    # Handle undefined receiver cases and assign values based on hitter\n",
    "    events_merge[\"receiver\"] = np.where((events_merge.receiver == \"__undefined__\") & (events_merge.hitter == \"Nadal\"), \"Djokovic\",\n",
    "                                       np.where((events_merge.receiver == \"__undefined__\") & (events_merge.hitter == \"Djokovic\"), \"Nadal\", \n",
    "                                                events_merge.receiver))\n",
    "\n",
    "    # Return relevant columns\n",
    "    return_cols = [\"rallyid\", \"frameid\", \"strokeid\", \"hitter\", \"receiver\", \"isserve\", \"serve\", \"type\", \"stroke\", \"hitter_x\", \"hitter_y\", \"receiver_x\", \"receiver_y\", \"time\"]\n",
    "    events_data_clean = events_merge[return_cols]\n",
    "    \n",
    "    return events_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659bbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.743290Z",
     "iopub.status.busy": "2025-02-05T00:13:17.742900Z",
     "iopub.status.idle": "2025-02-05T00:13:17.752759Z",
     "shell.execute_reply": "2025-02-05T00:13:17.751577Z"
    },
    "papermill": {
     "duration": 0.017707,
     "end_time": "2025-02-05T00:13:17.754692",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.736985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_bad_event_data_info(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Clean events dataset to correct found data issues in multiple columns\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event-level data for each rally\n",
    "\n",
    "    Output(s):\n",
    "        events_data_clean (pd.DataFrame): events dataset with uncovered data issues remedied   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove data that is an error upon review\n",
    "    events_data_clean = events_data[(events_data.rallyid != 5) & ~((events_data.rallyid == 11) & (events_data.strokeid == 1))]\n",
    "\n",
    "    # Change rally 101 strokes, rally id\n",
    "    events_data_clean.loc[events_data_clean.frameid == 157969, \"strokeid\"] = 16\n",
    "    events_data_clean.loc[events_data_clean.frameid == 157982, \"strokeid\"] = 17\n",
    "\n",
    "    events_data_clean.loc[events_data_clean.rallyid == 101, \"rallyid\"] = 100\n",
    "\n",
    "    # Change any undefined stroke column values\n",
    "    events_data_clean.loc[events_data_clean.stroke == \"__undefined__\", \"stroke\"] = \"forehand\"\n",
    "    \n",
    "    return events_data_clean\n",
    "\n",
    "def clean_bad_event_data_locations(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Clean events dataset to correct poor location data (e.g. Player is inside baseline when returning a serve)\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event-level data for each rally\n",
    "\n",
    "    Output(s):\n",
    "        events_data (pd.DataFrame): events dataset with poor location data for specific events fixed \n",
    "    \"\"\"\n",
    "\n",
    "    # Clean bad location data on serves/returns to be behind the baseline\n",
    "    events_data[\"receiver_y\"] = np.where(\n",
    "        (events_data.isserve) & (events_data.serve == \"second\") & \n",
    "            ((events_data.receiver_y > 1) & (events_data.receiver_y < 22) & (events_data.receiver_y > 12)), 24.0, \n",
    "        np.where(\n",
    "            (events_data.isserve) & (events_data.serve == \"first\") & ((events_data.receiver_y > 1) & \n",
    "                                                                               (events_data.receiver_y < 22) & (events_data.receiver_y > 12)), 28.0,\n",
    "            np.where(\n",
    "                (events_data.isserve) & (events_data.serve == \"second\") & \n",
    "                    ((events_data.receiver_y > 1) & (events_data.receiver_y < 22) & (events_data.receiver_y < 12)), -0.2,\n",
    "                np.where(\n",
    "                    (events_data.isserve) & (events_data.serve == \"first\") & \n",
    "                        ((events_data.receiver_y > 1) & (events_data.receiver_y < 22) & (events_data.receiver_y < 12)), -4,  \n",
    "                    events_data.receiver_y\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    events_data[\"hitter_y\"] = np.where(\n",
    "        (events_data.isserve) & ((events_data.hitter_y > 1) & (events_data.hitter_y < 22) & (events_data.hitter_y > 12)), 24.0, \n",
    "        np.where(\n",
    "            (events_data.isserve) & ((events_data.hitter_y > 1) & (events_data.hitter_y < 22) & (events_data.hitter_y < 12)), -0.2, \n",
    "            events_data.hitter_y\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    return events_data\n",
    "\n",
    "def clean_event_data(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Combine sub-functions to perform narrow data cleaning tasks to fully clean event data\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event-level data for each rally\n",
    "\n",
    "    Output(s):\n",
    "        event_data_cleaned_names_locations (pd.DataFrame): events dataset with data issues fixed\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean data errors with respect to individual events\n",
    "    event_data_cleaned_names = clean_bad_event_data_info(events_data)\n",
    "\n",
    "    # Clean errors with respect to locations\n",
    "    event_data_cleaned_names_locations = clean_bad_event_data_locations(event_data_cleaned_names)\n",
    "\n",
    "    return event_data_cleaned_names_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21d221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.766168Z",
     "iopub.status.busy": "2025-02-05T00:13:17.765759Z",
     "iopub.status.idle": "2025-02-05T00:13:17.771548Z",
     "shell.execute_reply": "2025-02-05T00:13:17.770632Z"
    },
    "papermill": {
     "duration": 0.013278,
     "end_time": "2025-02-05T00:13:17.773142",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.759864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_new_features_multioutput_model(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Read in events dataset to create features prior to training regression model\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event data prior to transformation\n",
    "\n",
    "    Output(s):\n",
    "        events_data (pd.DataFrame): Event data with added features of time between rally hit and type of rally hit\n",
    "    \"\"\"\n",
    "    # Create new feature: time difference from previous hit (seconds)\n",
    "    events_data[\"time_diff\"] = events_data['time'].diff()\n",
    "    events_data.loc[events_data['strokeid'] == 1, 'time_diff'] = 0\n",
    "    \n",
    "    # Combine serve columns and type column to create new column with all information\n",
    "    events_data[\"type_of_shot\"] = np.where(\n",
    "        (events_data.isserve) & (events_data.serve == \"first\") & (events_data.type == \"serve\"), \"first_serve\",\n",
    "        np.where(\n",
    "            (events_data.isserve) & (events_data.serve == \"second\") & (events_data.type == \"serve\"), \"second_serve\", events_data.type\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # For categorical columns in data, convert into type category\n",
    "    for col in ['type_of_shot', 'stroke']:\n",
    "        events_data[col] = events_data[col].astype('category')\n",
    "    \n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59e640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.784546Z",
     "iopub.status.busy": "2025-02-05T00:13:17.784138Z",
     "iopub.status.idle": "2025-02-05T00:13:17.790237Z",
     "shell.execute_reply": "2025-02-05T00:13:17.789168Z"
    },
    "papermill": {
     "duration": 0.013787,
     "end_time": "2025-02-05T00:13:17.791980",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.778193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_ohc_transformation(data, cols_to_encode, one_hot_encoder = None):\n",
    "    \"\"\"\n",
    "    Purpose: With raw dataset, transform input columns with one-hot encoder\n",
    "\n",
    "    Input(s): \n",
    "        data (pd.DataFrame): Raw dataset\n",
    "        cols_to_encode (list): contains names of column in input\n",
    "        one_hot_encoder (NoneType or Sklearn object): default None, creates or uses preexisting one-hot encoder object\n",
    "\n",
    "    Output(s):\n",
    "        preprocessed_data (NumPy array): Dataset with one-hot encoded columns\n",
    "        one_hot_encoder (Sklearn object): Newly created or existing OHC object\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if OHC object exists\n",
    "    if one_hot_encoder is None:\n",
    "        # Initialize OHC Object\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "        # Fit the encoder to the specified columns\n",
    "        encoded_columns = one_hot_encoder.fit_transform(data[cols_to_encode])\n",
    "\n",
    "    else:\n",
    "        # Fit the encoder to the specified columns\n",
    "        encoded_columns = one_hot_encoder.transform(data[cols_to_encode])\n",
    "        \n",
    "    # Convert encoded columns to DataFrame\n",
    "    encoded_cols = pd.DataFrame(encoded_columns, columns=one_hot_encoder.get_feature_names_out(cols_to_encode))\n",
    "        \n",
    "    # Concatenate encoded columns with the original DataFrame\n",
    "    preprocessed_data = np.array(pd.concat([data.drop(columns=cols_to_encode), encoded_cols], axis=1))    \n",
    "    \n",
    "    return(preprocessed_data, one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bef17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.803108Z",
     "iopub.status.busy": "2025-02-05T00:13:17.802727Z",
     "iopub.status.idle": "2025-02-05T00:13:17.849884Z",
     "shell.execute_reply": "2025-02-05T00:13:17.848854Z"
    },
    "papermill": {
     "duration": 0.055113,
     "end_time": "2025-02-05T00:13:17.852100",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.796987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute functions to clean events and points data\n",
    "events_clean_names = clean_rally_hitter_receiver_names(events)\n",
    "events_data_clean_evts = clean_event_data(events_clean_names)\n",
    "\n",
    "# Set new points, events data to cleaned versions\n",
    "points_clean = get_true_rally_times(events_data_clean_evts)\n",
    "events_clean = events_data_clean_evts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe435cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.863687Z",
     "iopub.status.busy": "2025-02-05T00:13:17.863266Z",
     "iopub.status.idle": "2025-02-05T00:13:17.895212Z",
     "shell.execute_reply": "2025-02-05T00:13:17.894229Z"
    },
    "papermill": {
     "duration": 0.040012,
     "end_time": "2025-02-05T00:13:17.897324",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.857312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Perform data transformations for multioutput model dataset\n",
    "events_multioutput_model_data = create_new_features_multioutput_model(events_clean)\n",
    "\n",
    "# Set columns to include in input \n",
    "model_data_cols = [\"strokeid\", \"type_of_shot\", \"stroke\", \"time_diff\", \"hitter_x\", \"hitter_y\", \"receiver_x\", \"receiver_y\"]\n",
    "y_cols = [\"receiver_x\", \"receiver_y\"]\n",
    "\n",
    "# Split dataset into train/validation dataset and prediction dataset (missing locations)\n",
    "train_data = events_multioutput_model_data[~events_multioutput_model_data.receiver_x.isna()][model_data_cols].reset_index(drop=True)\n",
    "\n",
    "train_X = train_data.drop(y_cols, axis = 1)\n",
    "train_y = train_data[y_cols]\n",
    "\n",
    "predict_data = events_multioutput_model_data[events_multioutput_model_data.receiver_x.isna()][model_data_cols].reset_index(drop=True)\n",
    "predict_X = predict_data.drop(y_cols, axis = 1)\n",
    "\n",
    "# Specify columns to be encoded\n",
    "columns_to_encode = ['type_of_shot', 'stroke']\n",
    "\n",
    "# OHC categorical columns for preprocessing\n",
    "X_tr, encoder = perform_ohc_transformation(train_X, columns_to_encode)\n",
    "y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a15f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:17.909032Z",
     "iopub.status.busy": "2025-02-05T00:13:17.908687Z",
     "iopub.status.idle": "2025-02-05T00:13:18.346477Z",
     "shell.execute_reply": "2025-02-05T00:13:18.345285Z"
    },
    "papermill": {
     "duration": 0.446031,
     "end_time": "2025-02-05T00:13:18.348552",
     "exception": false,
     "start_time": "2025-02-05T00:13:17.902521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize RandomForest Regressor Model\n",
    "multioutput_rforest = MultiOutputRegressor(RandomForestRegressor(min_samples_split = 4, max_depth = 4, random_state = 64))\n",
    "\n",
    "# Retrain model on entire dataset, predict missing receiver locations\n",
    "multioutput_rforest.fit(X_tr, y)\n",
    "\n",
    "# On dataset with missing values, transform categorical columns with OHC object \n",
    "predict_X_tr, encoder = perform_ohc_transformation(predict_X, columns_to_encode, one_hot_encoder = encoder)\n",
    "\n",
    "predict_data[[\"receiver_x\", \"receiver_y\"]] = multioutput_rforest.predict(predict_X_tr)\n",
    "\n",
    "# Using predictions, replace missing locations with predictions\n",
    "imputed_location_event_data = pd.merge(\n",
    "    events_clean[events_clean.receiver_x.isna()].drop(y_cols, axis = 1),\n",
    "    predict_data\n",
    ")\n",
    "\n",
    "nonimputed_location_event_data = events_clean[~events_clean.receiver_x.isna()]\n",
    "\n",
    "# Concat data with and without imputed locations\n",
    "clean_dataset_cols = [\"rallyid\", \"strokeid\", \"hitter\", \"receiver\", \"stroke\", \"type_of_shot\", \"hitter_x\", \"hitter_y\", \"receiver_x\", \"receiver_y\", \"time_diff\"]\n",
    "\n",
    "events_clean_final = pd.concat([imputed_location_event_data, nonimputed_location_event_data])[clean_dataset_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce29c1e",
   "metadata": {
    "papermill": {
     "duration": 0.004855,
     "end_time": "2025-02-05T00:13:18.358664",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.353809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Location Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bdf6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.370177Z",
     "iopub.status.busy": "2025-02-05T00:13:18.369711Z",
     "iopub.status.idle": "2025-02-05T00:13:18.378900Z",
     "shell.execute_reply": "2025-02-05T00:13:18.377790Z"
    },
    "papermill": {
     "duration": 0.016969,
     "end_time": "2025-02-05T00:13:18.380599",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.363630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_player_loc_data(events_data, points_data):\n",
    "    \"\"\"\n",
    "    Purpose: Reflect the server and receiver location points above y = 11.89, so that the server is on the side closest to the baseline in each row\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): DataFrame containing the location data for players\n",
    "        points_data (pd.DataFrame): DataFrame containing the identity of the server for each point\n",
    "\n",
    "    Output(s):\n",
    "        events_data (pd.DataFrame): Updated DataFrame with reflected location points for players\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge events and points dataset to create identity of server column\n",
    "    rally_server_cols = [\"rallyid\", \"server\"]\n",
    "    disqualifying_events = [\"ace\", \"double_fault\"]\n",
    "        \n",
    "    events_data = pd.merge(events_data, points_data[~points_data.reason.isin(disqualifying_events)][rally_server_cols], on = \"rallyid\")\n",
    "\n",
    "    # Reflection parameters\n",
    "    y_reflect_line = 11.89\n",
    "    x_reflect_line = 5.49\n",
    "\n",
    "    # Transform data into a single column for server, receiver locations\n",
    "    events_data[\"server_x_tr\"] = np.where(events_data.hitter == events_data.server, events_data.hitter_x, events_data.receiver_x)\n",
    "    events_data[\"server_y_tr\"] = np.where(events_data.hitter == events_data.server, events_data.hitter_y, events_data.receiver_y)\n",
    "    \n",
    "    events_data[\"receiver_x_tr\"] = np.where(events_data.hitter != events_data.server, events_data.hitter_x, events_data.receiver_x)\n",
    "    events_data[\"receiver_y_tr\"] = np.where(events_data.hitter != events_data.server, events_data.hitter_y, events_data.receiver_y)\n",
    "\n",
    "    # For location points of servers above y = 11.89\n",
    "    \n",
    "    # Reflect over x = 5.49\n",
    "    events_data[\"server_x_refl\"] = np.where(events_data.server_y_tr > y_reflect_line, 2 * x_reflect_line - events_data.server_x_tr, events_data.server_x_tr)\n",
    "\n",
    "    # Reflect over y = 11.89\n",
    "    events_data[\"server_y_refl\"] = np.where(events_data.server_y_tr > y_reflect_line, 2 * y_reflect_line - events_data.server_y_tr, events_data.server_y_tr)\n",
    "\n",
    "    # Perform the opposite for receivers, to standardize all location data as the server is closest to TV camera\n",
    "\n",
    "    # Reflect over x = 5.49\n",
    "    events_data[\"receiver_x_refl\"] = np.where(events_data.server_y_tr > y_reflect_line, 2 * x_reflect_line - events_data.receiver_x_tr, events_data.receiver_x_tr)\n",
    "\n",
    "    # Reflect over y = 11.89\n",
    "    events_data[\"receiver_y_refl\"] = np.where(events_data.server_y_tr > y_reflect_line, 2 * y_reflect_line - events_data.receiver_y_tr, events_data.receiver_y_tr)\n",
    "\n",
    "    # Remove redundant columns from data\n",
    "    redundant_cols = [\"hitter\", \"receiver\", \"server\", \"hitter_x\", \"hitter_y\", \"receiver_x\", \"receiver_y\", \n",
    "                      \"server_x_tr\", \"server_y_tr\", \"receiver_x_tr\", \"receiver_y_tr\"]\n",
    "    \n",
    "    events_data = events_data.drop(redundant_cols, axis = 1)\n",
    "\n",
    "    # Rename _refl columns\n",
    "    events_data = events_data.rename(columns = {\n",
    "        \"server_x_refl\": \"server_x\",\n",
    "        \"server_y_refl\": \"server_y\",\n",
    "        \"receiver_x_refl\": \"receiver_x\",\n",
    "        \"receiver_y_refl\": \"receiver_y\"\n",
    "    })\n",
    "    \n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1cbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.392186Z",
     "iopub.status.busy": "2025-02-05T00:13:18.391792Z",
     "iopub.status.idle": "2025-02-05T00:13:18.398421Z",
     "shell.execute_reply": "2025-02-05T00:13:18.397240Z"
    },
    "papermill": {
     "duration": 0.01458,
     "end_time": "2025-02-05T00:13:18.400350",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.385770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_distance_measures_from_baseline(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Create measures of distance from center of baseline for each player, conditional on the side of court each is on\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event data with transformed location data for server/receiver for each point\n",
    "\n",
    "    Output(s):\n",
    "        events_data (pd.DataFrame): Event data with server and receiver distances from own baseline added\n",
    "    \"\"\"\n",
    "\n",
    "    # Set value for height, width of court\n",
    "    height_court = 10.97\n",
    "    width_court = 11.89*2\n",
    "\n",
    "    # Create coordinate for center of each baseline\n",
    "    server_side_baseline_center = [height_court/2, 0]\n",
    "    receiver_side_baseline_center = [height_court/2, width_court]\n",
    "    \n",
    "    # For server and receiver locations (transformed), calculate distance from respective baseline with L2 Norm\n",
    "    server_locations = np.array(events_data[[\"server_x\", \"server_y\"]])\n",
    "    receiver_locations = np.array(events_data[[\"receiver_x\", \"receiver_y\"]])\n",
    "    \n",
    "    server_distances_from_baseline = np.linalg.norm((server_locations - [server_side_baseline_center]), axis = 1)\n",
    "    receiver_distances_from_baseline = np.linalg.norm((receiver_locations - [receiver_side_baseline_center]), axis = 1)\n",
    "    \n",
    "    events_data[\"server_distance_from_baseline\"] = server_distances_from_baseline\n",
    "    events_data[\"receiver_distance_from_baseline\"] = receiver_distances_from_baseline\n",
    "\n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75108b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.411743Z",
     "iopub.status.busy": "2025-02-05T00:13:18.411361Z",
     "iopub.status.idle": "2025-02-05T00:13:18.417336Z",
     "shell.execute_reply": "2025-02-05T00:13:18.416270Z"
    },
    "papermill": {
     "duration": 0.013886,
     "end_time": "2025-02-05T00:13:18.419315",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.405429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_distance_measures_from_previous_hit(events_data):\n",
    "    \"\"\"\n",
    "    Purpose: Create measures of distance from each player's previous hit, using their transformed location data\n",
    "\n",
    "    Input(s):\n",
    "        events_data (pd.DataFrame): Contains event data with transformed location data for server/receiver for each point\n",
    "\n",
    "    Output(s):\n",
    "        events_data (pd.DataFrame): Event data with server and receiver distances from previous location added\n",
    "    \"\"\"\n",
    "\n",
    "    # Create previous location variable\n",
    "    events_data[\"server_x_prev\"] = events_data['server_x'].shift(1)\n",
    "    events_data[\"server_y_prev\"] = events_data['server_y'].shift(1)\n",
    "    \n",
    "    events_data[\"receiver_x_prev\"] = events_data['receiver_x'].shift(1)\n",
    "    events_data[\"receiver_y_prev\"] = events_data['receiver_y'].shift(1)\n",
    "    \n",
    "    # Set all previous locations for serve strokes to NA\n",
    "    events_data.loc[events_data.strokeid == 1, 'server_x_prev'] = np.nan\n",
    "    events_data.loc[events_data.strokeid == 1, 'server_y_prev'] = np.nan\n",
    "    \n",
    "    events_data.loc[events_data.strokeid == 1, 'receiver_x_prev'] = np.nan\n",
    "    events_data.loc[events_data.strokeid == 1, 'receiver_y_prev'] = np.nan\n",
    "\n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04afcae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.430979Z",
     "iopub.status.busy": "2025-02-05T00:13:18.430608Z",
     "iopub.status.idle": "2025-02-05T00:13:18.479010Z",
     "shell.execute_reply": "2025-02-05T00:13:18.477899Z"
    },
    "papermill": {
     "duration": 0.056377,
     "end_time": "2025-02-05T00:13:18.480778",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.424401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform feature additions/transformations using location data\n",
    "events_data_transformed = transform_player_loc_data(events_clean_final, points_clean)\n",
    "events_data_transformed = create_distance_measures_from_baseline(events_data_transformed)\n",
    "events_data_transformed = create_distance_measures_from_previous_hit(events_data_transformed)\n",
    "\n",
    "events_data_transformed.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ace6f",
   "metadata": {
    "papermill": {
     "duration": 0.005166,
     "end_time": "2025-02-05T00:13:18.491911",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.486745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4789bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.504428Z",
     "iopub.status.busy": "2025-02-05T00:13:18.503955Z",
     "iopub.status.idle": "2025-02-05T00:13:18.508785Z",
     "shell.execute_reply": "2025-02-05T00:13:18.507590Z"
    },
    "papermill": {
     "duration": 0.013293,
     "end_time": "2025-02-05T00:13:18.510788",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.497495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline Model (\"Historical Average\") \n",
    "# reference: https://github.com/JeffSackmann/tennis_MatchChartingProject/blob/master/charting-m-points-2010s.csv\n",
    "print(f\"% of Points Won by Server (2010-2019): {round(0.6423463 * 100, 1)}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544aa0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.523718Z",
     "iopub.status.busy": "2025-02-05T00:13:18.523335Z",
     "iopub.status.idle": "2025-02-05T00:13:18.550921Z",
     "shell.execute_reply": "2025-02-05T00:13:18.549856Z"
    },
    "papermill": {
     "duration": 0.035955,
     "end_time": "2025-02-05T00:13:18.552718",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.516763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create variable for target outcome: If the player on serve won the point\n",
    "points_clean[\"ServerWinsPoint\"] = np.where(points_clean.server == points_clean.winner, 1, 0)\n",
    "\n",
    "# Join points dataset with rallyid and target outcome to dataset\n",
    "event_model_dataset = pd.merge(\n",
    "    events_data_transformed,\n",
    "    points_clean[['rallyid', 'ServerWinsPoint']]\n",
    ")\n",
    "\n",
    "event_model_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601bd380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T00:13:18.566435Z",
     "iopub.status.busy": "2025-02-05T00:13:18.566097Z",
     "iopub.status.idle": "2025-02-05T00:13:18.592227Z",
     "shell.execute_reply": "2025-02-05T00:13:18.591025Z"
    },
    "papermill": {
     "duration": 0.035433,
     "end_time": "2025-02-05T00:13:18.594327",
     "exception": false,
     "start_time": "2025-02-05T00:13:18.558894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "model_train_set = event_model_dataset[event_model_dataset.rallyid < 169]\n",
    "\n",
    "# Create test set to be the last 2 games on serve for each in the test set\n",
    "model_test_set = event_model_dataset[event_model_dataset.rallyid >= 169]\n",
    "\n",
    "model_train_set.to_csv(output_path + \"training_set.csv\", index = False)\n",
    "model_test_set.to_csv(output_path + \"test_set.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 130300,
     "sourceId": 311436,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.212048,
   "end_time": "2025-02-05T00:13:19.422910",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-05T00:13:10.210862",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
